\chapter{Implementation}\section{Integration with NPPC data repository}Initial implementation and spike work was conducted with the assumption that a network connection to the NPPC data repository would be established and maintained by the system. After a meeting with Dr Colin Sauze, it became apparent that the project would be hosted on a machine at the NPPC and that the hosting environment (discussed in section~\ref{hosting}) would have the data repository mounted as a network drive as default. This meant that the system could treat the repository as a local drive on the hosting environment, removing the requirement for the system to manage a network connection to the repository. In order to avoid transferring and re-hosting images from the repository it was necessary to add the repository as a static content resource within Spring such that the Tomcat web server driving the system could serve the images. This was simple to achieve within Spring by providing a custom implementation of the \texttt{WebMvcAutoConfigurationAdapter} class and its \texttt{addResourceHandlers} method where a file location could be defined as a resource and assigned a URL based resource handler. In the case of the images within the repository, they are handled from the base URL \texttt{/images}.The initial design involved using a recursive method to parse the various directories within the repository and create the various plants for an experiment from the directory structure. Performance was a noticeable issue with experiment initialisation taking an unreasonable amount of time to read plants from the repository. During troubleshooting this performance issue, an iterative approach that was tightly coupled to the file system structure was implemented for comparison purposes. This iterative approach was over twice as quick as all the tested and recommended recursive methods especially when checks on specific directories were required such as those needed to check for filtered image modalities.It was decided that the iterative approach offers enough of a difference in performance for the tight-coupling to be a fair trade off. However, changes were made to the directory structure of the repository soon after the implementation of the iterative approach. Discussions followed with Dr Colin Sauze who has oversight of the repository and it was confirmed that no further changes were planned since utilities he had written to interact with the repository were now also tightly coupled to the new structure. In the event that further changes need to be made, a recursive solution is provided within the source code (commented) of the project ensuring the project can function without much in the way of code change.\section{Graphing System}As part of the planning and design phases for the graphing based functionality in the system it was necessary to investigate and decide upon a framework or library that would allow the generation and display of graph based visualisations within the system. Having selected Plotly.js it was necessary to integrate the library into the system. Being a Javascript library made this fairly simple. However, in order to allow the user to configure certain settings that control the way the graphs are generated, a number of helper functions had to be implemented in Javascript to provide a simple interface onto the Plotly library from within the Graph pages on the site. The flexibility of Plotly and the wide range of graphing options that it provides meant that implementing a means for the user to configure it completely was impractical. The pages in the system are built using a responsive CSS design (responsive functionality provided by Bootstrap), in order to be compatible and maintain the responsive resizing of elements within the page, the graphs generated via Plotly would resize in proportion to the html element in which they were positioned. Unfortunately, even though the documentation for Plotly mentions dynamic resizing it was not possible to get the feature to work correctly within the system and the exact reason for this is still unknown. The result of this is that each graph generated currently has fixed dimensions. Having integrated the Plotly library into the system, the next implementation step was to provide it with data. This was achieved by providing a number of asynchronous Javascript functions that would send the user selected parameters for the graph to a method in the \texttt{GraphsPageController} class. The \texttt{GraphsPageController} takes advantage of the convenience annotation \texttt{@ResponseBody} provided by the Spring framework that converts the object returned by a controller class (specified using the \texttt{@Controller} annotation) into JSON format. This made the data returned to the asynchronous function compatible with Plotly and could be passed directly to the function creating the graph. The graph data itself is retrieved by the \texttt{GraphingManager} class using the user selected attributes to query the data layer for resulting Plants or PlantDays. Implementing functionality that would return plant results into the graph page when clicking on data points in the graph provided to be more complex than initially estimated. The added complexity was a direct result of a separate design decision to move metadata attributes to their own table within the database (discussed in section~\ref{db}). The difficulty was in building the correct query in JPA query language in order to return plants based on the key/value attribute pairs for two separate attributes, gaining familiarity with the syntax and experimenting with the various join query options eventually solved the issue.\section{Domain model implementation and ORM}The design for the domain model was relatively straight forward and usually implementing simple relationships between classes is trivial, however, since the project uses Hibernate ORM it was necessary to become familiar with some of the inner workings of such a framework during the implementation phase. The entity classes (\texttt{Experiment}, \texttt{Plant}, \texttt{PlantDay}, \texttt{Metadata} etc) within the system are mapped directly to database tables using annotations as discussed in section~\ref{db}. In order to retrieve information from the database it was necessary to provide methods to query the database, the preferred way of doing so is to use dedicated repository classes (database access objects or DAOs) to access the database. Initially the implementation of these DAO classes used a manual approach that had been tested during the prototyping phase, listing\ref{lst:dao1} shows this manual approach when retrieving Plant objects via their barcode. A query is build using a standard parametrised technique to avoid injection attacks since the barcode comes from the front end of the system in some cases. The Query object itself is built by invoking a Hibernate `session' retrieved using a static accessor method, the query is then run on the database and results returned.\lstjava\lstinputlisting[label={lst:dao1},caption=Manual Hibernate session handling]{sourceCode/mySnippets/impl/dao1.java}Unfortunately the approach used in  listing~\ref{lst:dao1} was not compatible with more complex database interactions especially, those involving lazy initialisation(as discussed in section~\ref{db}). Using the \texttt{getSession()} approach to retrieve a Plant would return a Plant proxy object with a lazy, non-initialised list of the PlantDays associated with the Plant. Attempts to read or manipulate PlantDay data within this lazy list would result in Hibernate exceptions where the session attempting to access the database is now closed since the session was scoped only within the \texttt{getPlantByBarcode} method. Attempts to maintain the same session for multiple queries and lazy initialisation proved impractical. Research showed that using the \texttt{@Transactional} annotation would enable the Spring framework to manage transactions within classes or methods using the annotation, in order to take advantage of the framework managed transactions it was necessary to completely refactor the persistence layer of the system.Whilst investigating the options for the persistence layer the \texttt{Repository} interfaces provided by Spring were discovered which provide a means to define queries in an extremely simple and efficient manner whilst also hooking into the transaction and session management provided by the \texttt{@Transactional} annotation. A particular extension of the \texttt{Repository} interface, the \texttt{JpaRepository}, was chosen as the base for all the domain DAOs in the system. The \texttt{JpaRepository} interface provides both CRUD functionality and provides a simple pagable function that allows pagination queries to be easily defined. Interestingly using these interfaces provides a means for the framework to infer database queries from method signatures alone, no implementation class is necessary beyond the custom implementation of the interface. \lstinputlisting[label={lst:dao2},caption=Interface query definition]{sourceCode/mySnippets/impl/dao2.java}Listing~\ref{lst:dao2} shows how the same functionality as in listing~\ref{lst:dao1} is achieved using the \texttt{JpaRepository} interface. There's no override or concrete implementation of the method, it is called directly from the interface and all functionality is inferred by Spring. For more complex queries the query can be built in the interface directly using the \texttt{@Query} annotation as shown in listing~\ref{lst:dao3}. Queries can be parametrised using the \texttt{@Param} annotation as shown.\lstinputlisting[label={lst:dao3},caption=Building custom hibernate queries by decorating interface methods]{sourceCode/mySnippets/impl/dao3.java}Changing to use the \texttt{JpaRepository} interfaces allowed abstraction away from the database sessions and individual transactions and allowed queries to be defined in a much more developer friendly and maintainable fashion. Using the \texttt{@Transactional} annotation instructs Spring to manage the commit and rollback functionality on the database further providing utility to the developer by providing innate rollback in the event of any error during a transaction. In solving the lazy initialisation issue discussed previously, it became apparent that the domain design calling for bi-directional relationships between one-to-many related entities would not be workable or necessary. The bi-directionality could be achieved using standard Java getter methods alone without needing to have this translated to the database via ORM. Having the bi-directionality at a database level caused circular instantiation to occur during initialisation of lazy collections within an entity object, this would manifest in stack overflow exceptions showing evidence of a poorly thought out design and was quickly rectified.\section{Data Import}Implementing the design for the experiment data import system was fairy straightforward. The design called for the use of custom annotations in the header of a source CSV file in order to efficiently route the contained data correctly. When the header of the CSV file is parsed, the header is separated from the body of the file, the columns corresponding to each annotation type have their index number added to a list corresponding to that type. For example, if the second column in the header was annotated with the \texttt{\{\{plant-t\}\}} (see section~\ref{dataimp} for annotation details) annotation, then the index `2' would be added to the list holding plant tag column indices, further columns with the same annotation would have their indices added to the same list. In a similar way, the column containing the identifying barcode for the Plants is identified and its index saved. The body of the experiment CSV file is processed line by line. A line in the file is represented by a list of String objects. For each line in the file, the identifying barcode is extracted by reading the value in the line list occupying the index corresponding to the barcode, the correct plant can then be found in the database. A similar approach is taken for the other column types, essentially their values are extracted by directly reading the value in the line that corresponds to the index in a given list. Listing~\ref{lst:dataIm} shows how this is achieved for plant data attributes, the indices corresponding to plant attribute columns are held in the list \texttt{plantAttribIndex}, for each index in this list the values are read directly from the line. In this instance the key/value attribute pair consists of the header and the column value, this attribute pair is then saved into the plant corresponding to the barcode found in the line.\lstjava\lstinputlisting[label={lst:dataIm},caption=Detail of routing data during import]{sourceCode/mySnippets/impl/header.java}Issues encountered during the implementation of this feature were mostly generic issues concerning the reading in of data from file, notably date formatting and certain special characters causing issues when attempting to display them as attributes or tags in the front end of the site since Spring and the page templating framework Thymeleaf have some inbuilt special character sanitisation.\section{IBERS hosted environment}\label{hosting}As discussed in section~\ref{framework}, a consideration during the selection of framework within which the project would be implemented was the number of dependencies it would place on the environment it was hosted. The less dependencies then the more attractive the framework since this directly affects the time required to configure and maintain the environment. This project required that three dependencies be present on a given environment to enable the system to function, the Java runtime environment, MySQL database server and the Apache Maven build tool which is bundled into and required to run Spring-Boot applications. Initially the Java version used for the project was version 8, however, it was more convenient for the NPPC to provide a server with Java version 7 forcing the change in targeted language level. Fortunately, this occurred sufficiently early in the implementation that not much of the codebase was dependant on new features not available in the version 7 Java runtime and the required changes were small and trivial.The provided hosting environment was provisioned and set up by Dr Colin Sauze, the data manager at the NPPC. The hosting environment is a virtual machine running Ubuntu v14.04 hosted on a Intel CPU based server with 8GB of RAM. It is hosted with the Universities firewall and as such is only accessible from within the University network (or via VPN). The network restriction meant that the deployment of a release build of the system could not be completed via the project continuous integration platform. The deployment process was not fully automated for the project. For a given release version, the source code would be checked out from the version control repository and the system rebuilt and restarted on the server itself. Since this required only three commands at the end of each week to be entered into the system terminal, it was deemed that automation was not necessary.